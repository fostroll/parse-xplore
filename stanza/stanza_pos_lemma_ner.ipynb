{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanza POS, LEMMA and NER Tagging Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Navigation:\n",
    "* [General Info](#info)\n",
    "* [Setting up Stanza for training](#setup)\n",
    "* [Training POS and LEMMA taggers with Stanza](#pos)\n",
    "* [Preparing Dataset for NER](#prepare)\n",
    "* [Adding BIOES Annotation](#bioes)\n",
    "* [Training NER tagger with Stanza](#ner)\n",
    "* [Using Trained Model for Prediction](#predict)\n",
    "* [Prediction and Saving to CONLL-U](#save)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Info <a class=\"anchor\" id=\"info\"></a>\n",
    "\n",
    "[`Link to Manual`](https://stanfordnlp.github.io/stanza/index.html) [`Training Page`](https://stanfordnlp.github.io/stanza/training.html)\n",
    "\n",
    "[`Link to GitHub Repository`](https://github.com/stanfordnlp/stanza) (git clone this repo)\n",
    "\n",
    "`Libraries needed:` `corpuscula.conllu` (conllu parsing); `stanza` (training); `json` (saving results); `tqdm` (displaying progress)\n",
    "\n",
    "`Pre-Trained Embeddings used in this example:` Recommended vectors are downloaded from [here](https://lindat.mff.cuni.cz/repository/xmlui/bitstream/handle/11234/1-1989/word-embeddings-conll17.tar?sequence=9&isAllowed=y)(~30GB, 60+ languages)\n",
    "\n",
    "`Pipeline Input:` CONLL-U parsed text file.\n",
    "\n",
    "`Processing:` Extracting tokens and named entities as separate lists of lists of strings, and adding BIOES tags to entities.\n",
    "\n",
    "`Train Input:` `{train,dev,test}.bio` files in BIOES format as shown [here](https://en.wikipedia.org/wiki/Inside–outside–beginning_(tagging))\n",
    "\n",
    "`Sample train input:`\n",
    "```\n",
    "здравствуйте O\n",
    "расскажите O\n",
    "справочной S-Department\n",
    "аэропорта S-Organization\n",
    "город B-Geo\n",
    "томск E-Geo\n",
    "```\n",
    "\n",
    "`Sample inference (predict) result:`\n",
    "```\n",
    ">> print(*[f'token: {token.text}\\tner: {token.ner}' for sent in doc.sentences for token in sent.tokens], sep='\\n')\n",
    "token: 4\tner: B-Organization\n",
    "token: больница\tner: I-Organization\n",
    "token: детская\tner: I-Organization\n",
    "token: городская\tner: I-Organization\n",
    "token: больница\tner: I-Organization\n",
    "token: номер\tner: I-Organization\n",
    "token: 4\tner: E-Organization\n",
    "token: города\tner: B-Geo\n",
    "token: сочи\tner: E-Geo\n",
    "token: приемный\tner: B-Department\n",
    "token: покой\tner: E-Department\n",
    "```\n",
    "\n",
    "`Pipeline Output:` JSON with NER, POS, Features Parsing (list of lists of dict)\n",
    "\n",
    "`Sample pipeline output:`\n",
    "```\n",
    "[[{'word': 'подскажите',\n",
    "   'entity': None,\n",
    "   'pos': 'VERB',\n",
    "   'feats': 'Aspect=Perf|Mood=Imp|Number=Plur|Person=2|VerbForm=Fin|Voice=Act'},\n",
    "  {'word': 'мне',\n",
    "   'entity': None,\n",
    "   'pos': 'PRON',\n",
    "   'feats': 'Case=Dat|Number=Sing|Person=1'},\n",
    "  {'word': 'регистратуру',\n",
    "   'entity': 'Department',\n",
    "   'pos': 'NOUN',\n",
    "   'feats': 'Animacy=Inan|Case=Acc|Gender=Fem|Number=Sing'}, ...]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Stanza for training <a class=\"anchor\" id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run in terminal.\n",
    "\n",
    "1. Clone Stanza GitHub repository\n",
    "```\n",
    "$git clone https://github.com/stanfordnlp/stanza\n",
    "```\n",
    "\n",
    "2. Move to cloned git repository & download embeddings ({lang}.vectors.xz format)\n",
    "(run in a screen, takes up about 5-6h)\n",
    "```\n",
    "$ cd stanza\n",
    "$ ./scripts/download_vectors.sh ./extern_data/\n",
    "```\n",
    "\n",
    "3. Make sure your `./stanza/scripts/config.sh` is set up like below. Modify if necessary (pay attention to UDBASE and NERBASE).\n",
    "\n",
    "```\n",
    "export UDBASE=./udbase\n",
    "\n",
    "export NERBASE=./nerbase\n",
    "\n",
    "# Set directories to store processed training/evaluation files\n",
    "export DATA_ROOT=./data\n",
    "export TOKENIZE_DATA_DIR=$DATA_ROOT/tokenize\n",
    "export MWT_DATA_DIR=$DATA_ROOT/mwt\n",
    "export POS_DATA_DIR=$DATA_ROOT/pos\n",
    "export LEMMA_DATA_DIR=$DATA_ROOT/lemma\n",
    "export DEPPARSE_DATA_DIR=$DATA_ROOT/depparse\n",
    "export ETE_DATA_DIR=$DATA_ROOT/ete\n",
    "export NER_DATA_DIR=$DATA_ROOT/ner\n",
    "export CHARLM_DATA_DIR=$DATA_ROOT/charlm\n",
    "\n",
    "# Set directories to store external word vector data\n",
    "export WORDVEC_DIR=./extern_data/word2vec\n",
    "```\n",
    "\n",
    "Prepared train-dev-test will be placed to `{NERBASE}/{corpus}/{train,dev,test}.bio`, where `{corpus}` = full language name (e.g. Russian). \n",
    "\n",
    "Stanza does not accept any alterations to corpus names.\n",
    "\n",
    "4. Download language resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stanza.download('ru')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training POS and LEMMA taggers with Stanza <a class=\"anchor\" id=\"pos\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`STEP 1`**\n",
    "\n",
    "`Input files for POS and LEMMA model training should be placed here:` \n",
    "\n",
    "**`{UDBASE}/{corpus}/{corpus_short}-ud-{train,dev,test}.conllu`**, where \n",
    "* **`{UDBASE}`** is `./stanza/udbase/` (specified in `config.sh`), \n",
    "* **`{corpus}`** is full corpus name (e.g. `UD_Russian-SynTagRus` or `UD_English-EWT`, case-sensitive), and \n",
    "* **`{corpus_short}`** is the treebank code, can be [found here](https://stanfordnlp.github.io/stanza/model_history.html) (e.g. `ru_syntagrus`).\n",
    "\n",
    "**`STEP 2`**\n",
    "\n",
    "**Important:** Create `./data/{pos,lemma}/` folder, otherwise the code below will fail to run.\n",
    "\n",
    "\n",
    "**`STEP 3`** To prepare data, run:\n",
    "```\n",
    "$ cd stanza\n",
    "$ ./scripts/prep_{pos,lemma}_data.sh UD_Russian-SynTagRus\n",
    "```\n",
    "The script above prepares the train-dev-test.conllu data which is located in `./udbase/UD_Russian-SynTagRus/`.\n",
    "\n",
    "**`STEP 4`**\n",
    "To start training, run:\n",
    "```\n",
    "$ ./scripts/run_{pos,lemma}.sh UD_Russian-SynTagRus\n",
    "```\n",
    "The model will be saved to `saved models`.\n",
    "\n",
    "**`HOW TO USE`** \n",
    "#### Loading Trained Models to Pipeline\n",
    "\n",
    "\n",
    "To load the model for prediction, when setting up Tagger Pipeline, specify path to the model:\n",
    "```\n",
    "nlp = stanza.Pipeline('ru', \n",
    "                       processors='tokenize,pos,lemma,ner',\n",
    "                       pos_model_path=<path to model>,\n",
    "                       lemma_model_path=<path to model>,\n",
    "                       ner_model_path=<path to model>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset for NER <a class=\"anchor\" id=\"prepare\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpuscula.conllu import Conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(corpus=None, silent=False):\n",
    "    if isinstance(corpus, str):\n",
    "        corpus = Conllu.load(corpus, **({'log_file': None} if silent else{}))\n",
    "    elif callable(corpus):\n",
    "        corpus = corpus()\n",
    "\n",
    "    parsed_corpus = []\n",
    "    parsed_ne = []\n",
    "    \n",
    "    for sent in corpus:\n",
    "        curr_sent = [x['FORM'] for x in sent[0] if x['FORM'] and '-' not in x['ID']]\n",
    "        curr_ne = [x['MISC']['NE'] if 'NE' in x['MISC'].keys() else 'O' for x in sent[0]]\n",
    "        parsed_corpus.append(curr_sent)\n",
    "        parsed_ne.append(curr_ne)\n",
    "    \n",
    "    return parsed_corpus, parsed_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[===============================] 30390                             \n",
      "Corpus has been loaded: 30390 sentences, 378829 tokens\n",
      "Load corpus\n",
      "[====] 3799                                                        \n",
      "Corpus has been loaded: 3799 sentences, 47280 tokens\n",
      "Load corpus\n",
      "[====] 3798                                                        \n",
      "Corpus has been loaded: 3798 sentences, 47126 tokens\n"
     ]
    }
   ],
   "source": [
    "# replace file names, if necessary\n",
    "parsed_corpus_train, named_entities_train = read_corpus('result_ner_train.conllu')\n",
    "parsed_corpus_dev, named_entities_dev = read_corpus('result_ner_dev.conllu')\n",
    "parsed_corpus_test, named_entities_test = read_corpus('result_ner_test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['добрый',\n",
       "   'день',\n",
       "   'девушка',\n",
       "   'скажите',\n",
       "   'пожалуйста',\n",
       "   'мне',\n",
       "   'телефончик',\n",
       "   'автобусная',\n",
       "   'да',\n",
       "   'по',\n",
       "   'бежецкого']],\n",
       " [['O', 'O', 'O', 'O', 'O', 'O', 'O', 'Organization', 'O', 'O', 'Address']])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_corpus_train[:1], named_entities_train[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding BIOES Annotation <a class=\"anchor\" id=\"bioes\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bioes_annotation(ne_list):    \n",
    "\n",
    "    # Adding BIOES-annotation for future training with Stanza \n",
    "    \n",
    "    prev_ne = 'O'\n",
    "    bioes_ne = []\n",
    "    \n",
    "    for i, ne in enumerate(ne_list):\n",
    "        if ne == 'O':\n",
    "            prev_ne = 'O'\n",
    "            \n",
    "        elif prev_ne == 'O' or ne != prev_ne.split('-')[1]:\n",
    "            if i < len(ne_list)-1 and ne == ne_list[i+1]:\n",
    "                ne = 'B-' + ne   \n",
    "            else:\n",
    "                ne = 'S-' + ne\n",
    "\n",
    "        elif ne == prev_ne.split('-')[1] and prev_ne.split('-')[0] in ['B', 'I']:\n",
    "            if i < len(ne_list)-1 and ne == ne_list[i+1]:\n",
    "                ne = 'I-' + ne\n",
    "            else:\n",
    "                ne = 'E-' + ne\n",
    "                    \n",
    "        prev_ne = ne\n",
    "        bioes_ne.append(ne)\n",
    "    \n",
    "    return bioes_ne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_ne_train = [bioes_annotation(ne_seq) for ne_seq in named_entities_train]\n",
    "bio_ne_dev = [bioes_annotation(ne_seq) for ne_seq in named_entities_dev]\n",
    "bio_ne_test = [bioes_annotation(ne_seq) for ne_seq in named_entities_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['O', 'O', 'O', 'O', 'O', 'O', 'O', 'S-Organization', 'O', 'O', 'S-Address']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_ne_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify paths and file names, if necessary\n",
    "import os\n",
    "\n",
    "# Note that \"Russian\" is considered as a name of the corpus. \n",
    "# You cannot give your corpus other names, only language names.\n",
    "nerbase = './stanza/nerbase/'\n",
    "corpus_lang = 'Russian'\n",
    "\n",
    "dn = os.path.join(nerbase, corpus_lang)\n",
    "if not os.path.isdir(dn):\n",
    "    os.makedirs(dn)\n",
    "\n",
    "with open(os.path.join(dn, 'train.bio'), 'wt', encoding='utf-8') as f:\n",
    "    for i in range(len(parsed_corpus_train)):\n",
    "        [print('\\n'.join([' '.join(pair) for pair in list(zip(parsed_corpus_train[i],\n",
    "                                                              bio_ne_train[i]))]),\n",
    "               file=f)]\n",
    "        print(file=f)\n",
    "\n",
    "with open(os.path.join(dn, 'dev.bio'), 'wt', encoding='utf-8') as f:\n",
    "    for i in range(len(parsed_corpus_dev)):\n",
    "        [print('\\n'.join([' '.join(pair) for pair in list(zip(parsed_corpus_dev[i],\n",
    "                                                              bio_ne_dev[i]))]),\n",
    "               file=f)] \n",
    "        print(file=f)\n",
    "        \n",
    "with open(os.path.join(dn, 'test.bio'), 'wt', encoding='utf-8') as f:\n",
    "    for i in range(len(parsed_corpus_test)):\n",
    "        [print('\\n'.join([' '.join(pair) for pair in list(zip(parsed_corpus_test[i],\n",
    "                                                              bio_ne_test[i]))]),\n",
    "               file=f)]\n",
    "        print(file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training NER tagger with Stanza <a class=\"anchor\" id=\"ner\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to terminal and run:\n",
    "```\n",
    "$ cd stanza\n",
    "$ ./scripts/run_ner.sh Russian --shorthand='ru_syntagrus'\n",
    "```\n",
    "\n",
    "Your model will be saved to `/stanza/saved_models/ner/ru_syntagrus_nertagger.pt`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Trained Model for Prediction  <a class=\"anchor\" id=\"predict\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to disable Stanza built-in tokenizer, specify `tokenize_pretokenized=True` parameter in Pipeline.\n",
    "\n",
    "Input should still be a list of strings, but tokens will be separated by spaces, no multi-word tokens will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import stanza\n",
    "from tqdm import tqdm\n",
    "\n",
    "def stanza_parse(sents,\n",
    "                 pos_model='./stanza/saved_models/pos/ru_syntagrus_tagger.pt',\n",
    "                 lemma_model='./stanza/saved_models/lemma/ru_syntagrus_lemmatizer.pt',\n",
    "                 ner_model='./stanza/saved_models/ner/ru_syntagrus_nertagger.pt'):\n",
    "    \n",
    "    sents = [' '.join(sent) for sent in sents]\n",
    "    nlp = stanza.Pipeline('ru',\n",
    "                          processors='tokenize,pos,lemma,ner',\n",
    "                          pos_model_path=pos_model,\n",
    "                          lemma_model_path=lemma_model,\n",
    "                          ner_model_path=ner_model,\n",
    "                          tokenize_pretokenized=True)\n",
    "    \n",
    "    for idx, sent in enumerate(tqdm(sents)):\n",
    "        doc = nlp(sent)       \n",
    "        res = []\n",
    "\n",
    "        assert len(doc.sentences) == 1, \\\n",
    "               'ERROR: incorrect lengths of sentences ({}) for sent {}' \\\n",
    "                   .format(len(doc.sentences), idx)\n",
    "        sent = doc.sentences[0]\n",
    "        tokens, words = sent.tokens, sent.words\n",
    "        assert len(tokens) == len(words), \\\n",
    "               'ERROR: inconsistent lengths of tokens and words for sent {}' \\\n",
    "                   .format(idx)\n",
    "        for token, word in zip(tokens, words):\n",
    "            res.append({\n",
    "                'ID': token.id,\n",
    "                'FORM': token.text,\n",
    "                'LEMMA': word.lemma,\n",
    "                'UPOS': word.upos,\n",
    "                'XPOS': word.xpos,\n",
    "                'FEATS': OrderedDict(\n",
    "                    [(k, v) for k, v in [\n",
    "                        t.split('=', 1) for t in word.feats.split('|')\n",
    "                    ]] if word.feats else []\n",
    "                ),\n",
    "                'HEAD': None,\n",
    "                'DEPREL': None,\n",
    "                'DEPS': None,\n",
    "                'MISC': OrderedDict(\n",
    "                    [('NE', token.ner[2:])] if token.ner != 'O' else []\n",
    "                )\n",
    "            })\n",
    "\n",
    "        yield res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction and Saving Results to CONLL-U  <a class=\"anchor\" id=\"save\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-04-04 12:08:55 INFO: Loading these models for language: ru (Russian):\n",
      "=======================================\n",
      "| Processor | Package                 |\n",
      "---------------------------------------\n",
      "| tokenize  | syntagrus               |\n",
      "| pos       | ./stanza/s..._tagger.pt |\n",
      "| lemma     | ./stanza/s...matizer.pt |\n",
      "| ner       | ./stanza/s...rtagger.pt |\n",
      "=======================================\n",
      "\n",
      "2020-04-04 12:08:55 INFO: Use device: gpu\n",
      "2020-04-04 12:08:55 INFO: Loading: tokenize\n",
      "2020-04-04 12:08:55 INFO: Loading: pos\n",
      "2020-04-04 12:08:59 INFO: Loading: lemma\n",
      "2020-04-04 12:08:59 INFO: Loading: ner\n",
      "2020-04-04 12:08:59 INFO: Done loading processors!\n",
      "100%|██████████| 3798/3798 [02:09<00:00, 29.40it/s]\n"
     ]
    }
   ],
   "source": [
    "Conllu.save(stanza_parse(parsed_corpus_test), 'stanza_syntagrus.conllu',\n",
    "            fix=True, log_file=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
