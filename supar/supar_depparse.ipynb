{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UXo_HC6GGJhz"
   },
   "source": [
    "# Dependency Parsing with SuPar\n",
    "\n",
    "Project repo: https://github.com/yzhangcs/parser\n",
    "\n",
    "Prerequisites:\n",
    "* `python`: 3.7\n",
    "* `pytorch`: 1.4\n",
    "* `transformers`: 3.0\n",
    "\n",
    "We recommend installing SuPar from GitHub repository. \n",
    "The `pip install supar` version did not allow us to train the models.\n",
    "In the Issues, the author as well recommends installing SuPar from source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## If not installed earlier:\n",
    "# !apt install git -y\n",
    "# !apt install wget -y\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "4ByBF6BuOlJi",
    "outputId": "63bf8b95-e478-49dd-a9fe-f784df86a511"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install corpuscula\n",
    "!pip install junky\n",
    "\n",
    "# At the time we tested SuPar, the `pip install` version didn't work, so we\n",
    "# clone the original repo with the latest version\n",
    "\n",
    "!git clone https://github.com/yzhangcs/parser\n",
    "!cd parser && python setup.py install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9iiFLARW7jik"
   },
   "outputs": [],
   "source": [
    "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1YV9L3AXORclrfGKuxh9LuARNhaUX5iri' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1YV9L3AXORclrfGKuxh9LuARNhaUX5iri\" -O model && rm -rf /tmp/cookies.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "colab_type": "code",
    "id": "QiVHGfrEO5e-",
    "outputId": "5b3a0c92-877b-4664-8088-8cbcf3a79547"
   },
   "outputs": [],
   "source": [
    "!wget http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize/ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsYN1dQfDGlM"
   },
   "source": [
    "## Loading corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xIToeo_dPzJM"
   },
   "outputs": [],
   "source": [
    "from corpuscula.corpus_utils import download_ud, syntagrus\n",
    "\n",
    "corpus_name = 'UD_Russian-SynTagRus'\n",
    "# corpus_name = 'UD_Russian-Taiga'\n",
    "download_ud(corpus_name, root_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VWmoEKhnD-FC"
   },
   "source": [
    "## Filtering embeddings\n",
    "\n",
    "SuPar requires a lot of CPU memory to load word embeddings. 25 Gb is not enough. If you have enough CPU memory, you can use SuPar without filtering embeddings.\n",
    "\n",
    "Otherwise, shrinking word vectors definitely helps while still showing comparable-to-SOTA results and taking no more than a couple of minutes. We implemented vector filtering in our `junky` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W6-y_fJ4CDxi"
   },
   "outputs": [],
   "source": [
    "import junky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177
    },
    "colab_type": "code",
    "id": "vD_piMMaDEv4",
    "outputId": "bb27d4e3-50be-4d6d-a021-a9cb19bec137"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Load corpus\n",
      "[=================================================] 48814           \n",
      "Corpus has been loaded: 48814 sentences, 871526 tokens\n",
      "Load corpus\n",
      "[=======] 6584                                                     \n",
      "Corpus has been loaded: 6584 sentences, 118692 tokens\n",
      "Load corpus\n",
      "[=======] 6491                                                     \n",
      "Corpus has been loaded: 6491 sentences, 117523 tokens\n"
     ]
    }
   ],
   "source": [
    "junky.clear_tqdm()\n",
    "fpath = 'corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-{}.conllu'\n",
    "\n",
    "train, train_lemmas = junky.get_conllu_fields(fpath.replace('{}', 'train'), fields=['LEMMA'])\n",
    "dev, dev_lemmas = junky.get_conllu_fields(fpath.replace('{}', 'dev'), fields=['LEMMA'])\n",
    "test, test_lemmas = junky.get_conllu_fields(fpath.replace('{}', 'test'), fields=['LEMMA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-oYlXcmEFFLX"
   },
   "outputs": [],
   "source": [
    "full_corpus = train + train_lemmas + dev + dev_lemmas + test + test_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "Cz0Kk0HoCRoa",
    "outputId": "84d4beb5-8903-453e-9e72-55f1d5fe3621"
   },
   "outputs": [],
   "source": [
    "# any pretrained word embeddings in txt format where first line is `<vocab_size> <emb_dim>`\n",
    "FT_VECTORS_PATH = 'ft_native_300_ru_wiki_lenta_nltk_wordpunct_tokenize.vec'\n",
    "\n",
    "_ = junky.filter_embeddings(pretrained_embs=FT_VECTORS_PATH, corpus=full_corpus,\n",
    "                      min_abs_freq=1, save_name='filtered_vectors_freq1.vec',\n",
    "                      pad_token='[PAD]', unk_token='[UNK]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biaffine Dependency Parsing\n",
    "\n",
    " [[Link]](https://arxiv.org/abs/1611.01734) Original Paper: Timothy Dozat and Christopher D. Manning. 2017. Deep Biaffine Attention for Neural Dependency Parsing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6eHJy8ac9RHN"
   },
   "source": [
    "### Training with w2v+Bert embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the GPU you have, you might have the following error while running the training script:\n",
    "```\n",
    "Error: mkl-service + Intel(R) MKL: MKL_THREADING_LAYER=INTEL is incompatible with libgomp.so.1 library.\n",
    "\tTry to import numpy first or set the threading layer accordingly. Set MKL_SERVICE_FORCE_INTEL to force it.\n",
    "```\n",
    "To avoid the problem, set the environment variable `MKL_SERVICE_FORCE_INTEL=1`. \n",
    "You can run the code below and then run training script withour restarting the notebook.\n",
    "```python\n",
    "import os\n",
    "os.environ[\"MKL_SERVICE_FORCE_INTEL\"] = \"1\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "BlXV6UMszHPE",
    "outputId": "82e52996-66d2-4a02-acaf-37ce0d1108dc"
   },
   "outputs": [],
   "source": [
    "!python -m supar.cmds.biaffine_dependency train -b -d 0    \\\n",
    "  -p ./model-bert \\\n",
    "  -f bert  \\\n",
    "  --punct  \\\n",
    "  --train corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu \\\n",
    "  --dev corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu \\\n",
    "  --test corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu \\\n",
    "  --embed filtered_vectors_freq1.vec \\\n",
    "  --unk '[UNK]' \\\n",
    "  --n-embed 300 \\\n",
    "  --batch-size 1000 \\\n",
    "  --bert bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "auH8pokv9dun"
   },
   "source": [
    "### Training with w2v+character embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "UsLMIlTWB_bg",
    "outputId": "e4c6db56-3552-42fe-b560-e993ad1eefba"
   },
   "outputs": [],
   "source": [
    "!python -m supar.cmds.biaffine_dependency train -b -d 0    \\\n",
    "  -p ./model-char \\\n",
    "  -f char  \\\n",
    "  --punct  \\\n",
    "  --train corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu \\\n",
    "  --dev corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu \\\n",
    "  --test corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu \\\n",
    "  --embed filtered_vectors_freq1.vec \\\n",
    "  --unk '[UNK]' \\\n",
    "  --n-embed 300 \\\n",
    "  --batch-size 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed training on several GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch \\\n",
    "  --nproc_per_node=4 --master_port=10000  \\\n",
    "  -m supar.cmds.biaffine_dependency train -b -d 0,1 \\\n",
    "  -p ./model-bert-distr \\\n",
    "  -f bert  \\\n",
    "  --punct  \\\n",
    "  --train corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-train.conllu \\\n",
    "  --dev corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-dev.conllu \\\n",
    "  --test corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu \\\n",
    "  --embed filtered_vectors_freq1.vec \\\n",
    "  --unk '[UNK]' \\\n",
    "  --n-embed 300 \\\n",
    "  --batch-size 1500 \\\n",
    "  --bert bert-base-multilingual-cased"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WYWfTOP6prLV"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supar import Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-07 13:32:45 INFO Load the data\n",
      "2020-08-07 13:32:51 INFO                                                           \n",
      "Dataset(n_sentences=6491, n_batches=26, n_buckets=8)\n",
      "2020-08-07 13:32:51 INFO Make predictions on the dataset\n",
      "100%|####################################| 26/26 00:04<00:00,  6.33it/s\n",
      "2020-08-07 13:32:56 INFO Save predicted results to supar_char_depparse_test.conllu\n",
      "2020-08-07 13:32:57 INFO 0:00:04.629948s elapsed, 1401.96 Sents/s\n"
     ]
    }
   ],
   "source": [
    "parser = Parser.load('model')\n",
    "test_dataset = parser.predict(test, pred='supar_char_depparse_test.conllu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "During evaluation, the model makes predictions on the target corpus first, and then runs evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-07 13:33:10 INFO Load the data\n",
      "2020-08-07 13:33:26 INFO                                                           \n",
      "Dataset(n_sentences=6491, n_batches=26, n_buckets=8)\n",
      "2020-08-07 13:33:26 INFO Evaluate the dataset\n",
      "2020-08-07 13:33:39 INFO loss: 0.2480 - UCM: 61.61% LCM: 50.41% UAS: 94.89% LAS: 92.79%\n",
      "2020-08-07 13:33:39 INFO 0:00:13.039016s elapsed, 497.81 Sents/s\n"
     ]
    }
   ],
   "source": [
    "parser = Parser.load('model-bert')\n",
    "loss, metric = parser.evaluate('corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-08-07 13:33:41 INFO Load the data\n",
      "2020-08-07 13:33:48 INFO                                                           \n",
      "Dataset(n_sentences=6491, n_batches=26, n_buckets=8)\n",
      "2020-08-07 13:33:48 INFO Evaluate the dataset\n",
      "2020-08-07 13:33:52 INFO loss: 0.2753 - UCM: 58.45% LCM: 47.97% UAS: 94.29% LAS: 92.15%\n",
      "2020-08-07 13:33:52 INFO 0:00:04.232478s elapsed, 1533.62 Sents/s\n"
     ]
    }
   ],
   "source": [
    "parser = Parser.load('model')\n",
    "loss, metric = parser.evaluate('corpus/_UD/UD_Russian-SynTagRus/ru_syntagrus-ud-test.conllu')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "supar_depparse.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
